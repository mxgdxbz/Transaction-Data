---
title: "Code_for_Final_Project"
author: "CHUHAN"
date: "11/14/2018"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
---

```{r setup}
library(tidyverse)
require(caTools)
library(ggplot2)
library(rsample)
library(arules)
library(dplyr)

data=read.csv("BlackFriday.csv") %>%
  janitor::clean_names() %>%
  rename(stay_years=stay_in_current_city_years)
data[c(5,8)]=lapply(data[c(5,8)],factor)
length(unique(data$user_id))
summary(data)

data.wide=data %>%
  select(-product_id,-product_category_1,-product_category_2,product_category_3) %>%
  group_by(user_id,gender,age,occupation,city_category,stay_years,marital_status) %>%
  summarise(purc.total=sum(purchase))


#check the distribution of response
par(mfcol=c(1,2))
hist(data.wide$purc.total,main="Histgram of raw data")
hist(log(data.wide$purc.total),main="Histgram of log-transformed data") #log-transformation
data.wide$log.purchase=log(data.wide$purc.total)
levels(data.wide$marital_status)=c("unmarried","married")

```

###Exploratory Analysis
```{r}
library(gridExtra)
occupation=data.wide %>%
  group_by (gender,occupation,marital_status) %>%
  summarise(mean.purc=mean(log.purchase))
p1=ggplot(data=occupation,aes(x=occupation,y=mean.purc,group=gender,color=gender))+geom_point()+geom_line()+ggtitle("Purchase Capacity vs. Occupation")+theme_bw()+facet_wrap(~marital_status);p1

age=data.wide %>%
  group_by(gender,age,marital_status) %>%
  summarise(mean.purc=mean(log.purchase))
p2=ggplot(data=age,aes(x=age,y=mean.purc,group=gender,color=gender))+geom_point()+geom_line()+ggtitle("Purchase Capacity vs. Age Group")+theme_bw()+theme(axis.text.x = element_text(angle = 90, hjust = 1))+facet_wrap(~marital_status);p2

city=data.wide %>%
  group_by(gender,city_category,marital_status) %>%
  summarise(mean.purc=mean(log.purchase))
p3=ggplot(data=city,aes(x=city_category,y=mean.purc,group=gender,color=gender))+geom_point()+geom_line()+ggtitle("Purchase Capacity vs. Current City")+theme_bw()+facet_wrap(~marital_status);p3

years=data.wide %>%
  group_by(gender,stay_years,marital_status) %>%
  summarise(mean.purc=mean(log.purchase))
p4=ggplot(data=years,aes(x=stay_years,y=mean.purc,group=gender,color=gender))+geom_point()+geom_line()+ggtitle("Purchase Capacity vs. Years of Stay")+theme_bw()+facet_wrap(~marital_status);p4

#product
product=data %>%
  group_by(product_id) %>%
  summarise(pu.sum=sum(purchase))
popular.prod=product %>% top_n(20)
prod.names=as.character(popular.prod$product_id)
product.filter=data %>%
  filter(product_id %in% prod.names) %>%
  mutate(log.purchase=log(purchase)) %>%
  group_by(product_id,gender) %>%
  summarise(mean.purc=mean(log.purchase))
p6=ggplot(data=product.filter,aes(x=product_id,y=mean.purc,group=gender,color=gender))+geom_point()+geom_line()+ggtitle("Purchase Power vs. Popular Products")+theme_bw()+theme(axis.text.x = element_text(angle = 90, hjust = 1))

grid.arrange(grobs=list(p1,p3),width=c(3:2))
grid.arrange(grobs=list(p2,p4))

```

### Association rules
```{r,eval=FALSE}
library(PRIMsrc)

# implements unsupervised discretization
data.wide$purchase_df =as.factor(discretize(data.wide[[8]],method = "cluster",breaks = 3))
data.wide$purchase_capacity=as.factor(
  ifelse(data.wide$purc.total<1.01e+06,"low",                               
         ifelse((data.wide$purc.total<2.67e+06),"mediate","high")))


transdata= as(data.wide[,c(2:7,11)], "transactions") 
inspect(transdata)
transdata=as(transdata, "data.frame")

rules <- apriori(transdata, parameter = list(minlen=2,supp = 0.1, conf = 0.8),
                 appearance =list(rhs=c("purchase_capacity=low","purchase_capacity=mediate","purchase_capacity=high")))
summary(rules)
rules.sorted <- sort(rules, by="lift")
inspect(rules.sorted)

library(arulesViz)
plot(rules, method="graph", control=list(type="items"))
```

### least squares regression
```{r, echo=FALSE}
library(boot)
set.seed(11)
cv.error=rep(0,10)
for (i in 1:10){
  glm.fit=glm(log.purchase~gender*marital_status+age+stay_years+occupation+city_category,data=data.wide)
  cv.error[i]=cv.glm(data.wide,glm.fit,K=10)$delta[1]
 
}
glm.mse=mean(cv.error)
glm.mse 
glm.coef=glm.fit$coefficients
sd.glm=sd((predict(glm.fit)- data.wide$log.purchase)^2)/sqrt(nrow(data.wide))
sd.glm

summary(glm.fit)
```

### lasso regression
```{r}
library(glmnet) 
# data partition 
split <- initial_split(data.wide, prop = .7)
train <- training(split)
test  <- testing(split)

x=model.matrix(log.purchase~gender*marital_status+age+stay_years+occupation+city_category, data=train[c(2:7,9)])[,-1]
x.test=model.matrix(log.purchase~gender*marital_status+age+stay_years+occupation+city_category, data=test[c(2:7,9)])[,-1]
y=train$log.purchase
y.test=test$log.purchase

# fit model
set.seed (11)
cv.lasso = cv.glmnet(x,y,alpha=1,nfolds=10)
cv.bestlam =cv.lasso$lambda.min
cv.bestlam #0.01178443
cv.selam=cv.lasso$lambda.1se
cv.selam #0.05730299

set.seed (11)
lasso.cv.model= glmnet(x,y,alpha=1, lambda=cv.bestlam) 
predict.lasso.cv=predict(lasso.cv.model,newx=x.test)
mse.lasso1=mean((predict.lasso.cv - y.test)^2);mse.lasso1 # 0.8673
sd.lasso1=sd((predict.lasso.cv - y.test)^2)/sqrt(nrow(test));sd.lasso1 #0.0226625
lasso.coef1=as.matrix(coef(lasso.cv.model))

lasso.cv.model= glmnet(x,y,alpha=1, lambda=cv.selam) 
predict.lasso.cv=predict(lasso.cv.model,newx=x.test)
mse.lasso2=mean((predict.lasso.cv - y.test)^2);mse.lasso2 # 0.8796646
sd.lasso2=sd((predict.lasso.cv - y.test)^2)/sqrt(nrow(test));sd.lasso2 #0.02239355
lasso.coef2=as.matrix(coef(lasso.cv.model))

lasso=data.frame(cbind(cv.lasso$nzero,cv.lasso$lambda,cv.lasso$cvm,cv.lasso$cvlo,cv.lasso$cvup)) 
colnames(lasso) = c("size","lambda","cvm","cvlo","cvup")

ggplot(lasso,aes(x=log(lambda),y=cvm)) + geom_point(color="orange") + geom_errorbar(aes(ymin=cvlo, ymax=cvup), width=0.1,color="lightblue") + geom_line(color="orange",group=1) + geom_hline(yintercept = lasso[which(lasso$lambda==cv.bestlam),]$cvup,linetype=2,color=6) + geom_vline(xintercept = log(cv.selam),linetype=2,color=6) + theme_light() + labs(x="log(lambda)",y="CV Error",title="Lasso regression, 10-fold cross-validation")
```

### Principal Components Regression
```{r}
library(pls)
set.seed(11)
# Perform PCR on the training data and evaluate its test set performance.
pcr.fit=pcr(log.purchase~gender*marital_status+age+stay_years+occupation+city_category,data=train,scale=TRUE, validation = "CV" )#scaling by standard deviation for every segment

# find the best number of components and choose k
validationplot(pcr.fit ,val.type="MSEP" ) 
itemp=which.min(pcr.fit$validation$PRESS);itemp 

pcr.fit=pcr(log.purchase~gender*marital_status+age+occupation+city_category+stay_years,data=train,scale=TRUE, validation = "CV",ncomp=itemp)
summary(pcr.fit)

predict.pcr=predict(pcr.fit,x.test,ncomp=itemp) 
pcr.coef=c(0,pcr.fit$coefficients[,,itemp])
mse.pcr=mean((predict.pcr - y.test)^2);mse.pcr # 0.8707155
se.pcr=sd((predict.pcr - y.test)^2)/sqrt(nrow(test));se.pcr  #0.02311919
```

### model selection
```{r}
coef.matrix=cbind(glm.coef,lasso.coef1,lasso.coef2,pcr.coef)
colnames(coef.matrix)=c("least squares","lasso(lambda.min)","lasso(lambda.1se","pcr")
MSE=c(glm.mse,mse.lasso1,mse.lasso2,mse.pcr)
coef.matrix=rbind(coef.matrix,MSE)
knitr::kable(coef.matrix)
```

###Clustering analysis on products
```{r}
#filter popular products
trans.wide=data %>%
  select(-product_category_1,-product_category_2,-product_category_3)%>% 
  filter(product_id %in% prod.names) %>%
  spread(key=product_id,value=purchase,fill=0)


#Hierarchical Clustering
x=trans.wide[8:27]
sd.x=scale(x)
distance <- as.dist(1-cor(sd.x)) #convcerts to correlation-based distance matrix

#average linkage
hc.average =hclust(distance, method ="average")
hc.clusters=cutree(hc.average,4)
table(hc.clusters)
par(mfrow =c(1,3))
plot(hc.average, main="Average Linkage", xlab="", sub="",ylab="")
rect.hclust(hc.average,k=4)

#complete linkage
hc.complete =hclust(distance, method ="complete")
hc.clusters=cutree(hc.complete,4)
table(hc.clusters)
plot(hc.complete, main="Complete Linkage", xlab="", sub="",ylab="")
rect.hclust(hc.complete,k=4)

sub.id=trans.wide%>%
  filter(P00080342!=0)%>%
  select(user_id) %>%
  unlist()

consumer=data.wide%>%
  filter(user_id %in% sub.id)
summary(consumer)
```

###Principal Components Analysis
PCA looks to Ô¨Ånd a low-dimensional representation of the observations that explain a good fraction of the variance
```{r}
#Principal Components Analysis
pr.out=prcomp(t(sd.x))
hc.out.pc=hclust(dist(pr.out$x[ ,1:10]),
                 method="complete")
hc.clusters=cutree(hc.out.pc,4)
table(hc.clusters)

plot(hc.out.pc, 
     main="Principle Components",
     xlab="", sub="",ylab="")
rect.hclust(hc.out.pc,k=4)

```

```{r,include=FALSE}
### Gradient Boosting
library(gbm)
set.seed(11)
gbm_split <- initial_split(data.wide, prop = .7)
gbm_train <- training(gbm_split)
gbm_test  <- testing(gbm_split)
```

###Perform a grid search which iterates over every combination of hyperparameter values and allows us to assess which combination tends to perform well.
```{r,include=FALSE}
# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 7, 10),
  bag.fraction = c(0.7, .85, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)
# randomize data
random_index <- sample(1:nrow(gbm_train), nrow(gbm_train))
random_ames_train <- gbm_train[random_index, ]
# grid search 
for( i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = purc.total~gender+age+occupation+city_category+stay_years+marital_status,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)

# train a cross validated model using parameters specified above
gbm.fit.final <- gbm(
  purc.total~gender+age+occupation+city_category+stay_years+marital_status,
  data=gbm_test,
  distribution = "gaussian",
  n.trees = 500,
  interaction.depth = 5, #ensemble a bunch of stumps
  shrinkage = 0.05,
  cv.folds = 5,
  n.minobsinnode = 10,
  bag.fraction=0.70,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
print(gbm.fit.final)
summary(gbm.fit.final)

par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit.final, 
  cBars = 10,
  #method = relative.influence, 
  method=permutation.test.gbm,
  las = 2
  )

pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, gbm_test)

# results
caret::RMSE(pred, gbm_test$purc.total)
```



